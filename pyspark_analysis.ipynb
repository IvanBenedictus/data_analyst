{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize PySpark\n",
    "spark = SparkSession.builder.appName(\"Dataframe\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----+------+------+---+------+----+\n",
      "|total_bill| tip|   sex|smoker|day|  time|size|\n",
      "+----------+----+------+------+---+------+----+\n",
      "|     16.99|1.01|Female|    No|Sun|Dinner|   2|\n",
      "|     10.34|1.66|  Male|    No|Sun|Dinner|   3|\n",
      "|     21.01| 3.5|  Male|    No|Sun|Dinner|   3|\n",
      "|     23.68|3.31|  Male|    No|Sun|Dinner|   2|\n",
      "+----------+----+------+------+---+------+----+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import data from PySpark\n",
    "df_pyspark = spark.read.csv(\"./other_data/tips_dataset.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# Show first 4 rows\n",
    "df_pyspark.show(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- total_bill: double (nullable = true)\n",
      " |-- tip: double (nullable = true)\n",
      " |-- sex: string (nullable = true)\n",
      " |-- smoker: string (nullable = true)\n",
      " |-- day: string (nullable = true)\n",
      " |-- time: string (nullable = true)\n",
      " |-- size: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check for data typers\n",
    "df_pyspark.dtypes\n",
    "\n",
    "# Check for schema\n",
    "df_pyspark.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+------------------+------+------+----+------+------------------+\n",
      "|summary|       total_bill|               tip|   sex|smoker| day|  time|              size|\n",
      "+-------+-----------------+------------------+------+------+----+------+------------------+\n",
      "|  count|              243|               242|   242|   243| 243|   243|               243|\n",
      "|   mean|19.76617283950618| 3.001900826446281|  NULL|  NULL|NULL|  NULL|  2.57201646090535|\n",
      "| stddev|8.915417545076668|1.3878517469281784|  NULL|  NULL|NULL|  NULL|0.9523561732459417|\n",
      "|    min|             3.07|               1.0|Female|    No| Fri|Dinner|                 1|\n",
      "|    max|            50.81|              10.0|  Male|   Yes|Thur| Lunch|                 6|\n",
      "+-------+-----------------+------------------+------+------+----+------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Descrie PySparks data\n",
    "df_pyspark.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modify Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----+------+------+---+------+----+------------+\n",
      "|total_bill| tip|   sex|smoker|day|  time|size|amount_spend|\n",
      "+----------+----+------+------+---+------+----+------------+\n",
      "|     16.99|1.01|Female|    No|Sun|Dinner|   2|        18.0|\n",
      "|     10.34|1.66|  Male|    No|Sun|Dinner|   3|        12.0|\n",
      "|     21.01| 3.5|  Male|    No|Sun|Dinner|   3|       24.51|\n",
      "|     23.68|3.31|  Male|    No|Sun|Dinner|   2|       26.99|\n",
      "+----------+----+------+------+---+------+----+------------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Add new column\n",
    "df_pyspark.withColumn(\"amount_spend\", df_pyspark[\"total_bill\"]+df_pyspark[\"tip\"]).show(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----+------+------+---+------+----+\n",
      "|total_spend| tip|   sex|smoker|day|  time|size|\n",
      "+-----------+----+------+------+---+------+----+\n",
      "|      16.99|1.01|Female|    No|Sun|Dinner|   2|\n",
      "|      10.34|1.66|  Male|    No|Sun|Dinner|   3|\n",
      "|      21.01| 3.5|  Male|    No|Sun|Dinner|   3|\n",
      "|      23.68|3.31|  Male|    No|Sun|Dinner|   2|\n",
      "+-----------+----+------+------+---+------+----+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Rename column\n",
    "df_pyspark.withColumnRenamed(\"total_bill\", \"total_spend\").show(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----+------+------+---+------+\n",
      "|total_bill| tip|   sex|smoker|day|  time|\n",
      "+----------+----+------+------+---+------+\n",
      "|     16.99|1.01|Female|    No|Sun|Dinner|\n",
      "|     10.34|1.66|  Male|    No|Sun|Dinner|\n",
      "|     21.01| 3.5|  Male|    No|Sun|Dinner|\n",
      "|     23.68|3.31|  Male|    No|Sun|Dinner|\n",
      "+----------+----+------+------+---+------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Delete column\n",
    "df_pyspark.drop(\"size\").show(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handle Null Value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----+------+------+----+------+----+\n",
      "|total_bill| tip|   sex|smoker| day|  time|size|\n",
      "+----------+----+------+------+----+------+----+\n",
      "|     16.99|1.01|Female|    No| Sun|Dinner|   2|\n",
      "|     10.34|1.66|  Male|    No| Sun|Dinner|   3|\n",
      "|     21.01| 3.5|  Male|    No| Sun|Dinner|   3|\n",
      "|     23.68|3.31|  Male|    No| Sun|Dinner|   2|\n",
      "|      NULL|3.61|Female|    No| Sun|  NULL|   4|\n",
      "|     25.29|4.71|  NULL|    No| Sun|Dinner|   4|\n",
      "|      8.77|NULL|  NULL|    No|NULL|Dinner|NULL|\n",
      "|     26.88|NULL|  Male|  NULL| Sun|Dinner|   4|\n",
      "+----------+----+------+------+----+------+----+\n",
      "only showing top 8 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show sample datasets with null values\n",
    "df_pyspark.show(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---+---+------+---+----+----+\n",
      "|total_bill|tip|sex|smoker|day|time|size|\n",
      "+----------+---+---+------+---+----+----+\n",
      "|         1|  2|  2|     1|  1|   1|   1|\n",
      "+----------+---+---+------+---+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, isnan, when, count\n",
    "\n",
    "# Count how many null values in each column\n",
    "df_pyspark.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in df_pyspark.columns]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----+------+------+----+------+----+\n",
      "|total_bill| tip|   sex|smoker| day|  time|size|\n",
      "+----------+----+------+------+----+------+----+\n",
      "|      NULL|3.61|Female|    No| Sun|  NULL|   4|\n",
      "|     25.29|4.71|  NULL|    No| Sun|Dinner|   4|\n",
      "|      8.77|NULL|  NULL|    No|NULL|Dinner|NULL|\n",
      "|     26.88|NULL|  Male|  NULL| Sun|Dinner|   4|\n",
      "+----------+----+------+------+----+------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from functools import reduce\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Show datasets that have null values\n",
    "cols = [F.col(c) for c in df_pyspark.columns]\n",
    "filter_null = reduce(lambda a, b: a | b.isNull(), cols[1:], cols[0].isNull())\n",
    "\n",
    "df_pyspark.filter(filter_null).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----+------+------+---+------+----+\n",
      "|total_bill| tip|   sex|smoker|day|  time|size|\n",
      "+----------+----+------+------+---+------+----+\n",
      "|     16.99|1.01|Female|    No|Sun|Dinner|   2|\n",
      "|     10.34|1.66|  Male|    No|Sun|Dinner|   3|\n",
      "|     21.01| 3.5|  Male|    No|Sun|Dinner|   3|\n",
      "|     23.68|3.31|  Male|    No|Sun|Dinner|   2|\n",
      "|     15.04|1.96|  Male|    No|Sun|Dinner|   2|\n",
      "|     14.78|3.23|  Male|    No|Sun|Dinner|   2|\n",
      "|     10.27|1.71|  Male|    No|Sun|Dinner|   2|\n",
      "|     35.26| 5.0|Female|    No|Sun|Dinner|   4|\n",
      "+----------+----+------+------+---+------+----+\n",
      "only showing top 8 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Drop datasets that have any null values\n",
    "df_pyspark.na.drop(how=\"any\").show(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----+------+------+---+------+----+\n",
      "|total_bill| tip|   sex|smoker|day|  time|size|\n",
      "+----------+----+------+------+---+------+----+\n",
      "|     16.99|1.01|Female|    No|Sun|Dinner|   2|\n",
      "|     10.34|1.66|  Male|    No|Sun|Dinner|   3|\n",
      "|     21.01| 3.5|  Male|    No|Sun|Dinner|   3|\n",
      "|     23.68|3.31|  Male|    No|Sun|Dinner|   2|\n",
      "|     25.29|4.71|  NULL|    No|Sun|Dinner|   4|\n",
      "|     15.04|1.96|  Male|    No|Sun|Dinner|   2|\n",
      "|     14.78|3.23|  Male|    No|Sun|Dinner|   2|\n",
      "|     10.27|1.71|  Male|    No|Sun|Dinner|   2|\n",
      "+----------+----+------+------+---+------+----+\n",
      "only showing top 8 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Drop datasets that doesn't have at least 6 non null values\n",
    "df_pyspark.na.drop(how=\"any\", thresh=6).show(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----+------+------+---+------+----+\n",
      "|total_bill| tip|   sex|smoker|day|  time|size|\n",
      "+----------+----+------+------+---+------+----+\n",
      "|     16.99|1.01|Female|    No|Sun|Dinner|   2|\n",
      "|     10.34|1.66|  Male|    No|Sun|Dinner|   3|\n",
      "|     21.01| 3.5|  Male|    No|Sun|Dinner|   3|\n",
      "|     23.68|3.31|  Male|    No|Sun|Dinner|   2|\n",
      "|      NULL|3.61|Female|    No|Sun|  NULL|   4|\n",
      "|     25.29|4.71|  NULL|    No|Sun|Dinner|   4|\n",
      "|     15.04|1.96|  Male|    No|Sun|Dinner|   2|\n",
      "|     14.78|3.23|  Male|    No|Sun|Dinner|   2|\n",
      "+----------+----+------+------+---+------+----+\n",
      "only showing top 8 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Drop datasets that have any nukk value on column tip\n",
    "df_pyspark.na.drop(how=\"any\", subset=[\"tip\"]).show(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----+------+------+----+------+----+\n",
      "|total_bill| tip|   sex|smoker| day|  time|size|\n",
      "+----------+----+------+------+----+------+----+\n",
      "|     16.99|1.01|Female|    No| Sun|Dinner|   2|\n",
      "|     10.34|1.66|  Male|    No| Sun|Dinner|   3|\n",
      "|     21.01| 3.5|  Male|    No| Sun|Dinner|   3|\n",
      "|     23.68|3.31|  Male|    No| Sun|Dinner|   2|\n",
      "|      NULL|3.61|Female|    No| Sun|  NULL|   4|\n",
      "|     25.29|4.71|  NULL|    No| Sun|Dinner|   4|\n",
      "|      8.77| 0.0|  NULL|    No|NULL|Dinner|NULL|\n",
      "|     26.88| 0.0|  Male|  NULL| Sun|Dinner|   4|\n",
      "+----------+----+------+------+----+------+----+\n",
      "only showing top 8 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Change null value to -\n",
    "df_pyspark.na.fill(0.0, [\"tip\"]).show(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----+------+------+----+------+----+\n",
      "|total_bill| tip|   sex|smoker| day|  time|size|\n",
      "+----------+----+------+------+----+------+----+\n",
      "|     16.99|1.01|Female|    No| Sun|Dinner|   2|\n",
      "|     10.34|1.66|  Male|    No| Sun|Dinner|   3|\n",
      "|     21.01| 3.5|  Male|    No| Sun|Dinner|   3|\n",
      "|     23.68|3.31|  Male|    No| Sun|Dinner|   2|\n",
      "|     19.77|3.61|Female|    No| Sun|  NULL|   4|\n",
      "|     25.29|4.71|  NULL|    No| Sun|Dinner|   4|\n",
      "|      8.77| 3.0|  NULL|    No|NULL|Dinner|   2|\n",
      "|     26.88| 3.0|  Male|  NULL| Sun|Dinner|   4|\n",
      "+----------+----+------+------+----+------+----+\n",
      "only showing top 8 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import mean\n",
    "from pyspark.sql.functions import round\n",
    "\n",
    "# Calculate the mean value for each column\n",
    "mean_values = df_pyspark.select(*(round(mean(col), 2).alias(col) for col in [\"total_bill\", \"tip\", \"size\"])).collect()[0].asDict()\n",
    "df_pyspark.na.fill(mean_values).show(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----+------+------+----+------+----+------------------+-----------+------------+\n",
      "|total_bill| tip|   sex|smoker| day|  time|size|total_bill_imputed|tip_imputed|size_imputed|\n",
      "+----------+----+------+------+----+------+----+------------------+-----------+------------+\n",
      "|     16.99|1.01|Female|    No| Sun|Dinner|   2|             16.99|       1.01|           2|\n",
      "|     10.34|1.66|  Male|    No| Sun|Dinner|   3|             10.34|       1.66|           3|\n",
      "|     21.01| 3.5|  Male|    No| Sun|Dinner|   3|             21.01|        3.5|           3|\n",
      "|     23.68|3.31|  Male|    No| Sun|Dinner|   2|             23.68|       3.31|           2|\n",
      "|      NULL|3.61|Female|    No| Sun|  NULL|   4|             17.78|       3.61|           4|\n",
      "|     25.29|4.71|  NULL|    No| Sun|Dinner|   4|             25.29|       4.71|           4|\n",
      "|      8.77|NULL|  NULL|    No|NULL|Dinner|NULL|              8.77|       2.88|           2|\n",
      "|     26.88|NULL|  Male|  NULL| Sun|Dinner|   4|             26.88|       2.88|           4|\n",
      "+----------+----+------+------+----+------+----+------------------+-----------+------------+\n",
      "only showing top 8 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import Imputer\n",
    "from pyspark.sql.functions import round\n",
    "\n",
    "# Using imputation to find mean or median\n",
    "imputer = Imputer(\n",
    "    inputCols=[\"total_bill\", \"tip\", \"size\"], \n",
    "    outputCols=[\"{}_imputed\".format(c) for c in [\"total_bill\", \"tip\", \"size\"]]).setStrategy(\"median\")\n",
    "\n",
    "imputer.fit(df_pyspark).transform(df_pyspark).show(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pyspark = df_pyspark.na.drop(how=\"any\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter Operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----+------+------+----+------+----+\n",
      "|total_bill| tip|   sex|smoker| day|  time|size|\n",
      "+----------+----+------+------+----+------+----+\n",
      "|     48.27|6.73|  Male|    No| Sat|Dinner|   4|\n",
      "|     40.17|4.73|  Male|   Yes| Fri|Dinner|   4|\n",
      "|      44.3| 2.5|Female|   Yes| Sat|Dinner|   3|\n",
      "|     41.19| 5.0|  Male|    No|Thur| Lunch|   5|\n",
      "|     48.17| 5.0|  Male|    No| Sun|Dinner|   6|\n",
      "|     50.81|10.0|  Male|   Yes| Sat|Dinner|   3|\n",
      "|     45.35| 3.5|  Male|   Yes| Sun|Dinner|   3|\n",
      "|     40.55| 3.0|  Male|   Yes| Sun|Dinner|   2|\n",
      "|     43.11| 5.0|Female|   Yes|Thur| Lunch|   4|\n",
      "|     48.33| 9.0|  Male|    No| Sat|Dinner|   4|\n",
      "+----------+----+------+------+----+------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Filter rows that have total_bill >= 40.0\n",
    "df_pyspark.filter(df_pyspark[\"total_bill\"] >= 40.0).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----+------+\n",
      "|total_bill| tip|  time|\n",
      "+----------+----+------+\n",
      "|     48.27|6.73|Dinner|\n",
      "|     40.17|4.73|Dinner|\n",
      "|      44.3| 2.5|Dinner|\n",
      "|     41.19| 5.0| Lunch|\n",
      "|     48.17| 5.0|Dinner|\n",
      "|     50.81|10.0|Dinner|\n",
      "|     45.35| 3.5|Dinner|\n",
      "|     40.55| 3.0|Dinner|\n",
      "|     43.11| 5.0| Lunch|\n",
      "|     48.33| 9.0|Dinner|\n",
      "+----------+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Filter certain rows that have total_bill >= 40.0\n",
    "df_pyspark.filter(df_pyspark[\"total_bill\"] >= 40.0).select([\"total_bill\", \"tip\", \"time\"]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----+------+------+----+------+----+\n",
      "|total_bill| tip|   sex|smoker| day|  time|size|\n",
      "+----------+----+------+------+----+------+----+\n",
      "|     48.27|6.73|  Male|    No| Sat|Dinner|   4|\n",
      "|     41.19| 5.0|  Male|    No|Thur| Lunch|   5|\n",
      "|     48.17| 5.0|  Male|    No| Sun|Dinner|   6|\n",
      "|     50.81|10.0|  Male|   Yes| Sat|Dinner|   3|\n",
      "|     43.11| 5.0|Female|   Yes|Thur| Lunch|   4|\n",
      "|     48.33| 9.0|  Male|    No| Sat|Dinner|   4|\n",
      "+----------+----+------+------+----+------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Filter rows that have total_bill >= 40 and tip >= 5.0\n",
    "df_pyspark.filter((df_pyspark[\"total_bill\"] >= 40.0) & (df_pyspark[\"tip\"] >= 5.0)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----+------+------+---+------+----+\n",
      "|total_bill| tip|   sex|smoker|day|  time|size|\n",
      "+----------+----+------+------+---+------+----+\n",
      "|     16.99|1.01|Female|    No|Sun|Dinner|   2|\n",
      "|     10.34|1.66|  Male|    No|Sun|Dinner|   3|\n",
      "|     21.01| 3.5|  Male|    No|Sun|Dinner|   3|\n",
      "|     23.68|3.31|  Male|    No|Sun|Dinner|   2|\n",
      "+----------+----+------+------+---+------+----+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Filter rows that doesn't tip >= 5.0\n",
    "df_pyspark.filter(~(df_pyspark[\"tip\"] >= 5.0)).show(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aggregate Value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------------------+------------------+---------+\n",
      "| Day|   sum(total_bill)|          sum(tip)|sum(size)|\n",
      "+----+------------------+------------------+---------+\n",
      "|Thur|1096.3299999999997|            171.83|      152|\n",
      "| Sun|1541.6300000000003|233.95000000000007|      202|\n",
      "| Sat|1778.3999999999996|             260.4|      219|\n",
      "| Fri|325.87999999999994|             51.96|       40|\n",
      "+----+------------------+------------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Sum function group by day\n",
    "df_pyspark.groupBy(\"Day\").sum().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------------------+-----------------+------------------+\n",
      "| Day|   avg(total_bill)|         avg(tip)|         avg(size)|\n",
      "+----+------------------+-----------------+------------------+\n",
      "|Thur|17.682741935483865|2.771451612903226|2.4516129032258065|\n",
      "| Sun| 21.41152777777778|3.249305555555557|2.8055555555555554|\n",
      "| Sat|20.441379310344825|2.993103448275862|2.5172413793103448|\n",
      "| Fri|17.151578947368417|2.734736842105263|2.1052631578947367|\n",
      "+----+------------------+-----------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Mean function group by day\n",
    "df_pyspark.groupBy(\"Day\").mean().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+\n",
      "| Day|count|\n",
      "+----+-----+\n",
      "|Thur|   62|\n",
      "| Sun|   72|\n",
      "| Sat|   87|\n",
      "| Fri|   19|\n",
      "+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Count function group by day\n",
    "df_pyspark.groupBy(\"Day\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+----------+-----------------+\n",
      "|         avg(size)|count(day)|  sum(total_bill)|\n",
      "+------------------+----------+-----------------+\n",
      "|2.5541666666666667|       240|4742.240000000001|\n",
      "+------------------+----------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.agg({\"total_bill\":\"sum\", \"size\":\"average\", \"day\": \"count\"}).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----+------+------+---+------+----+-----------+--------------+-----------+----------+\n",
      "|total_bill| tip|   sex|smoker|day|  time|size|sex_indexed|smoker_indexed|day_indexed|time_index|\n",
      "+----------+----+------+------+---+------+----+-----------+--------------+-----------+----------+\n",
      "|     16.99|1.01|Female|    No|Sun|Dinner|   2|        1.0|           0.0|        1.0|       0.0|\n",
      "|     10.34|1.66|  Male|    No|Sun|Dinner|   3|        0.0|           0.0|        1.0|       0.0|\n",
      "|     21.01| 3.5|  Male|    No|Sun|Dinner|   3|        0.0|           0.0|        1.0|       0.0|\n",
      "|     23.68|3.31|  Male|    No|Sun|Dinner|   2|        0.0|           0.0|        1.0|       0.0|\n",
      "|     15.04|1.96|  Male|    No|Sun|Dinner|   2|        0.0|           0.0|        1.0|       0.0|\n",
      "+----------+----+------+------+---+------+----+-----------+--------------+-----------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "# Indexing the data\n",
    "indexer = StringIndexer(inputCols=[\"sex\", \"smoker\", \"day\", \"time\"],\n",
    "                        outputCols=[\"sex_indexed\", \"smoker_indexed\", \"day_indexed\", \"time_index\"]) \n",
    "\n",
    "df_regression = indexer.fit(df_pyspark).transform(df_pyspark) \n",
    "df_regression.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----+------+------+---+------+----+-----------+--------------+-----------+----------+--------------------+\n",
      "|total_bill| tip|   sex|smoker|day|  time|size|sex_indexed|smoker_indexed|day_indexed|time_index|Independent Features|\n",
      "+----------+----+------+------+---+------+----+-----------+--------------+-----------+----------+--------------------+\n",
      "|     16.99|1.01|Female|    No|Sun|Dinner|   2|        1.0|           0.0|        1.0|       0.0|[1.01,2.0,1.0,0.0...|\n",
      "|     10.34|1.66|  Male|    No|Sun|Dinner|   3|        0.0|           0.0|        1.0|       0.0|[1.66,3.0,0.0,0.0...|\n",
      "|     21.01| 3.5|  Male|    No|Sun|Dinner|   3|        0.0|           0.0|        1.0|       0.0|[3.5,3.0,0.0,0.0,...|\n",
      "|     23.68|3.31|  Male|    No|Sun|Dinner|   2|        0.0|           0.0|        1.0|       0.0|[3.31,2.0,0.0,0.0...|\n",
      "|     15.04|1.96|  Male|    No|Sun|Dinner|   2|        0.0|           0.0|        1.0|       0.0|[1.96,2.0,0.0,0.0...|\n",
      "+----------+----+------+------+---+------+----+-----------+--------------+-----------+----------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import VectorAssembler \n",
    "\n",
    "# Group all independent feature values into one array called Independet Features\n",
    "feature = VectorAssembler(inputCols=[\"tip\",\"size\",\"sex_indexed\",\"smoker_indexed\",\"day_indexed\", \"time_index\"],\n",
    "                                outputCol=\"Independent Features\") \n",
    "\n",
    "df_regression = feature.transform(df_regression)\n",
    "\n",
    "df_regression.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+\n",
      "|Independent Features|total_bill|\n",
      "+--------------------+----------+\n",
      "|[1.01,2.0,1.0,0.0...|     16.99|\n",
      "|[1.66,3.0,0.0,0.0...|     10.34|\n",
      "|[3.5,3.0,0.0,0.0,...|     21.01|\n",
      "|[3.31,2.0,0.0,0.0...|     23.68|\n",
      "|[1.96,2.0,0.0,0.0...|     15.04|\n",
      "+--------------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Final data include independet features and dependet feature (total_bill)\n",
    "final_data = output.select(\"Independent Features\",\"total_bill\")\n",
    "final_data.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(DenseVector([2.909, 3.5168, -0.9058, 2.1246, -0.2534, -1.1765]),\n",
       " 2.125411627178693)"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.regression import LinearRegression \n",
    "\n",
    "# Train and test split \n",
    "train_data, test_data = final_data.randomSplit([0.75,0.25]) \n",
    "regressor = LinearRegression(featuresCol='Independent Features', labelCol='total_bill') \n",
    "regressor = regressor.fit(train_data)\n",
    "\n",
    "regressor.coefficients, regressor.intercept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+------------------+\n",
      "|Independent Features|total_bill|        prediction|\n",
      "+--------------------+----------+------------------+\n",
      "|(6,[0,1],[1.45,2.0])|      9.55| 13.37700008281707|\n",
      "|(6,[0,1],[1.47,2.0])|     10.77|13.435180109218228|\n",
      "| (6,[0,1],[2.0,3.0])|     16.31|18.493719079626118|\n",
      "|(6,[0,1],[2.64,3.0])|     17.59|20.355479924463207|\n",
      "|(6,[0,1],[2.72,2.0])|     13.28|17.071431759290668|\n",
      "+--------------------+----------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Prediction result\n",
    "pred_results = regressor.evaluate(test_data)\n",
    "pred_results.predictions.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.5927212319671727, 4.555080156179097, 42.71725247682671)"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Performance result metric\n",
    "pred_results.r2, pred_results.meanAbsoluteError, pred_results.meanSquaredError"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
